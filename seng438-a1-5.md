>   **SENG 438 - Software Testing, Reliability, and Quality**

**Lab. Report \#1 â€“ Introduction to Testing and Defect Tracking**

| Group: 5      |
|-----------------|
| Mohammed Zaid Shaikh                |
| Alexander Lai              |
| Odin Fox               |
| Aidan Gaede-Janke               |


# Table of Contents


[1 Introduction	1](#introduction)

[2 High-level description of the exploratory testing plan	1](#high-level-description-of-the-exploratory-testing-plan)

[3 Comparison of exploratory and manual functional testing	1](#comparison-of-exploratory-and-manual-functional-testing)

[4 Notes and discussion of the peer reviews of defect reports	1](#notes-and-discussion-of-the-peer-reviews-of-defect-reports)

[5 How the pair testing was managed and team work/effort was divided 1](#how-the-pair-testing-was-managed-and-team-workeffort-was-divided)

[6 Difficulties encountered, challenges overcome, and lessonlearned	1](#difficulties-encountered-challenges-overcome-and-lessons-learned)

[7 Comments/feedback on the lab and lab document itself	1](#commentsfeedback-on-the-lab-and-lab-document-itself)

# Introduction

This lab aims to test the system using specific unit tests that target the application's main functionalities. Before this lab, we were familiar with basic software testing fundamentals, such as exploratory and manual function testing. Exploratory testing entails ad hoc and unscripted testing, where testers use their experience and intuition to find inherent flaws in the system without using pre-made tests. Manual function testing is a more thoughtful way to consider multiple cases using pre-established tests to confirm that the system behaved as expected.

Our goal in this lab was to apply manual function testing to an ATM simulation system to further our understanding. This will help to identify the benefits and challenges of this testing method while gaining hands-on experience with manual function testing.

# High-level description of the exploratory testing plan

Our exploratory testing aimed to find as many flaws as possible in the ATM simulation system (System Under Test, or SUT) by mimicking real user interactions. In addition to assessing error handling for invalid inputs, incorrect PINs, and system constraints, our methodology tested essential features, including login, withdrawals, deposits, transfers, and balance inquiries. Our goal was to evaluate the system's overall stability and user interface responsiveness in various scenarios.

We employed a procedure that compromises extensive coverage and targeted testing. First, we ensured that every significant feature was tested at least once. Then, we focused on high-risk domains, including cash handling, transaction processing, and PIN validation. We also examined edge instances, such as boundary values like the withdrawal limit and improper inputs like incorrect PIN entries.

Every testing couple conducted a 30-minute exploration session in which they freely engaged with the system and noted any issues they encountered. These flaws were entered into the defect tracking system with thorough descriptions, reproduction instructions, anticipated and actual outcomes, and severity levels. Thanks to the insights gathered from this step, we were able to identify possible system vulnerabilities. This methodical yet adaptable methodology also enabled us to quickly identify important flaws and obtain practical expertise with real-world testing methods.

# Comparison of exploratory and manual functional testing

The difference between exploratory and manual functional testing is the information known beforehand. When performing exploratory testing, the exact objective of the testing is not stated, whereas manual functional testing is when specific use cases are given to test. Exploratory testing is useful for testing smaller projects or evaluating user experience. Manual function testing is useful for thoroughly testing the limits of a code or targeting a particular function.

# Notes and discussion of the peer reviews of defect reports

We began with peer reviewing after all the tests were completed by both pairs on our team. It was helpful to have a second party come in and verify the bugs that we found within the system. It was also important to make sure that the bugs within the system weren't caused by the device that the system was running on.

# How the pair testing was managed and team work/effort was divided

Our team of 4 was split into 2 pairs, each pair handling the testing of different use cases. One pair (Alex, Zaid) handled testing for the deposit, transfer, inquiry, and PIN extension use cases. The other pair (Aidan, Odin) handled testing for the system startup, system shutdown, transaction, and withdrawal use cases, as well as some general session testing.
While performing exploratory testing, the tester of the pair kept track of the exact actions they took in navigating the software, allowing them to communicate the exact method to recreate bugs, when encountered, so that the other of the pair could record, in detail, how to reproduce issues.
Bugged or working, the reviewer was responsible to take note of the functionality of each tested element and organise results into the given test cases.

# Difficulties encountered, challenges overcome, and lessons learned

The main challenge we encountered was identifying edge cases while performing exploratory testing, which was solved by systematically working through each rudimentary test case and identifying any that crashed the system or caused it to become unresponsive. We were also able to put our theoretical understanding of testing into practice. We learned about how the testing process goes, and we were able to gauge for ourselves the effectiveness of explorative testing on unfamiliar software. By working hands-on, we were able to develop a more practical understanding of testing from the mostly theoretical understanding we had coming into this lab. Additionally, we gained hands-on experience with a bug tracking tool (Jira) that allowed us to accurately map bugs along with the specific version of the program. This notable experience will allow us to use Jira for our projects in other courses as well as in the industry to make bug tracking more efficient and collaborative.

# Comments/feedback on the lab and lab document itself

Overall, this lab provided a good overview of various software testing methods and resources. The practical method reinforced important ideas, especially in regression and exploratory testing. However, occasionally, the procedure itself felt tiresome, particularly when running the same test cases repeatedly and thoroughly recording any flaws. Although helpful, manual issue tracking and regression testing were monotonous and annoying.
